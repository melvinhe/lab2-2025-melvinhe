{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Matrix Tiling and Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the Fibertree emulator to display the behavior of various matrix multiplications for dense data. Because the data is dense we use the position-based operators as for dense data the position and coordinate are the same.\n",
    "\n",
    "In Einsum notation we will be computing the following, same as in Part 1:\n",
    "\n",
    "$$\n",
    "C_{m,n} = \\sum_k A_{m,k} \\times B_{k,n}\n",
    "$$\n",
    "\n",
    "In particular, we will observe how __matrix tiling__ changes the memory access pattern. Fibertree is a concept that will be covered later in class. Here, it is only used to provide an intuitive visualization of matrix tiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run boilerplate code to set up environment\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "from loaders import *\n",
    "\n",
    "from cache import Cache, CacheAssoc\n",
    "\n",
    "%run ./prelude.py --style=uncompressed --animation=movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication Input Selections\n",
    "\n",
    "Tiling is expressed by splitting the shape of each rank into two factors, e.g., M1 and M0, where the product is the full shape of the dimension, e.g., M = M1 x M0.\n",
    "\n",
    "We provide a widget below to control the tiling choice for your learning, but **PLEASE ANSWER ALL QUESTIONS WHILE USING THE PROVIDED INITIAL VALUES**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initial values\n",
    "M1 = 2\n",
    "M0 = 2\n",
    "K1 = 2\n",
    "K0 = 2\n",
    "N1 = 2\n",
    "N0 = 2\n",
    "\n",
    "density = [1.0]\n",
    "seed = 10\n",
    "\n",
    "enable_log = False\n",
    "\n",
    "def set_params(**kwargs):\n",
    "    global enable_log\n",
    "    \n",
    "    for variable, value in kwargs.items():\n",
    "        globals()[variable] = value\n",
    "\n",
    "    enable_log = (kwargs[\"log\"] == 'enable')\n",
    "\n",
    "\n",
    "def logger(arg):\n",
    "    if enable_log:\n",
    "        print(arg)\n",
    "\n",
    "controls = interactive(set_params,\n",
    "                       M0=widgets.IntSlider(min=1, max=4, step=1, value=M0),\n",
    "                       K0=widgets.IntSlider(min=1, max=4, step=1, value=K0),\n",
    "                       N0=widgets.IntSlider(min=1, max=4, step=1, value=N0),\n",
    "                       seed=widgets.IntSlider(min=0, max=100, step=1, value=seed),\n",
    "                       log=['disable', 'enable'])\n",
    "\n",
    "display(controls)\n",
    "createRunallButton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Input Tensors\n",
    "\n",
    "Given the shapes selected above, the following codeblock creates and displays the filter weights (**f**) and input activations (**i**) and a reference output (**o_verify**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 4\n",
    "M1 = M // M0\n",
    "K = 4\n",
    "K1 = K // K0\n",
    "N = 4\n",
    "N1 = N // N0\n",
    "\n",
    "a_MK_raw = []\n",
    "for m in range(M):\n",
    "    a_MK_raw.append([random.randint(1, 9) for i in range(K)])\n",
    "                 \n",
    "b_KN_raw = []\n",
    "for k in range(K):\n",
    "    b_KN_raw.append([random.randint(1, 9) for i in range(N)])\n",
    "\n",
    "a_MK = Tensor.fromUncompressed([\"M\", \"K\"], a_MK_raw)\n",
    "b_KN = Tensor.fromUncompressed([\"K\", \"N\"], b_KN_raw)\n",
    "\n",
    "a_MK.setName(\"A_MK\").setColor(\"blue\")\n",
    "b_KN.setName(\"B_KN\").setColor(\"green\")\n",
    "\n",
    "print(\"Input A\")\n",
    "displayTensor(a_MK)\n",
    "                    \n",
    "print(\"Input B\")\n",
    "displayTensor(b_KN)\n",
    "\n",
    "z_verify = None\n",
    "\n",
    "def create_z():\n",
    "    \"\"\"\n",
    "    Create a fully populated z tensor\n",
    "    \"\"\"\n",
    "    z = Tensor(rank_ids=[\"M\", \"N\"], default='')\n",
    "    z.setName(\"Z\")\n",
    "    z.setMutable(True)\n",
    "\n",
    "    z_m = z.getRoot()\n",
    "    #\n",
    "    # Hack to fill in all the entries in z\n",
    "    # This allows us to pretend the tensor is dense\n",
    "    #\n",
    "    n_fiber = Fiber(coords=range(N), initial=1)\n",
    "    m_fiber = Fiber(coords=range(M), initial=1)\n",
    "\n",
    "    for m, (z_n, _) in z_m << m_fiber:\n",
    "        for n, (z_ref, _) in z_n << n_fiber:\n",
    "            z_ref <<= 0\n",
    "            \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function for addressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAddress(tensor, x, y):\n",
    "    return (x*tensor.getShape()[1]+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are visualizing matrix multiplication we examined in Part 1 Question 2. You will observe some cache statistics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matrix Multiply\")\n",
    "\n",
    "cache_a = Cache(log_size=3, words_per_line=1)\n",
    "cache_b = Cache(log_size=3, words_per_line=1)\n",
    "cache_c = Cache(log_size=3, words_per_line=1)\n",
    "\n",
    "z_MN = create_z()\n",
    "\n",
    "print(\"Output - before\")\n",
    "displayTensor(z_MN)\n",
    "\n",
    "z = z_MN.getRoot()\n",
    "a = a_MK.getRoot()\n",
    "b = b_KN.getRoot()\n",
    "\n",
    "canvas = createCanvas(a_MK, b_KN, z_MN)\n",
    "for m in range(M):\n",
    "    a_tile = [ (m, kt) for kt in range(K)]\n",
    "    for n in range(K):\n",
    "        logger(f\"Processing Z({m},{n}) = {z[m][n]}\")\n",
    "        b_tile = [ (kt, n) for kt in range(K)]\n",
    "        z_tile = (m, n)\n",
    "        for k in range(N):\n",
    "            logger(f\"Processing A({m},{k}) = {a[m][k].payload}\")\n",
    "            logger(f\"Processing B({k},{n}) = {b[k][n].payload}\")\n",
    "            \n",
    "            cache_a.load(getAddress(a, m, k))\n",
    "            cache_b.load(getAddress(b, k, n))\n",
    "              \n",
    "            cache_c.load(getAddress(z, m, n))\n",
    "            z[m][n] += a[m][k] * b[k][n]\n",
    "            addActivity(canvas, a_tile, b_tile, z_tile, worker=\"W\")\n",
    "            addFrame(canvas, (m,k), (k,n), (m,n))\n",
    "        \n",
    "            cache_c.store(getAddress(z, m, n))\n",
    "\n",
    "print(\"Output - after\")\n",
    "displayTensor(z)\n",
    "\n",
    "displayCanvas(canvas)\n",
    "\n",
    "if z_verify is None:\n",
    "    z_verify = deepcopy(z)\n",
    "\n",
    "# Print cache statistics\n",
    "print(\"-------Cache A--------\")\n",
    "cache_a.print_stats()\n",
    "print(\"-------Cache B--------\")\n",
    "cache_b.print_stats()\n",
    "print(\"-------Cache C--------\")\n",
    "cache_c.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE ANSWER ALL QUESTIONS WHILE USING THE PROVIDED INITIAL VALUES\n",
    "answer(\n",
    "    question='2.1',\n",
    "    subquestion=\"Does the cache for tensor A have (1) compulsory, (2) capacity, and (3) conflict misses? Please write your answer (True/False in Python) in order as a list (e.g., [True, False, True]).\",\n",
    "    answer= ['FILL ME', 'FILL ME', 'FILL ME'], # Answer here\n",
    "    required_type=[bool, bool, bool],\n",
    ")\n",
    "answer(\n",
    "    question='2.1',\n",
    "    subquestion=\"Does the cache for tensor B have (1) compulsory, (2) capacity, and (3) conflict misses? Please write your answer (True/False in Python) in order as a list (e.g., [True, False, True]).\",\n",
    "    answer= ['FILL ME', 'FILL ME', 'FILL ME'], # Answer here\n",
    "    required_type=[bool, bool, bool],\n",
    ")\n",
    "answer(\n",
    "    question='2.1',\n",
    "    subquestion=\"Does the cache for tensor C have (1) compulsory, (2) capacity, and (3) conflict misses? Please write your answer (True/False in Python) in order as a list (e.g., [True, False, True]).\",\n",
    "    answer= ['FILL ME', 'FILL ME', 'FILL ME'], # Answer here\n",
    "    required_type=[bool, bool, bool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiled Matrix Multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are simulating a __tiled__ matrix multiply. Caches are the same as above. You do not need to code anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matrix Multiply\")\n",
    "\n",
    "cache_a = Cache(log_size=3, words_per_line=1)\n",
    "cache_b = Cache(log_size=3, words_per_line=1)\n",
    "cache_c = Cache(log_size=3, words_per_line=1)\n",
    "\n",
    "z_MN = create_z()\n",
    "\n",
    "print(\"Output - before\")\n",
    "displayTensor(z_MN)\n",
    "\n",
    "z = z_MN.getRoot()\n",
    "a = a_MK.getRoot()\n",
    "b = b_KN.getRoot()\n",
    "\n",
    "canvas = createCanvas(a_MK, b_KN, z_MN)\n",
    "\n",
    "for m1 in range(M1):\n",
    "    for k1 in range(K1):\n",
    "        for n1 in range(N1):\n",
    "            a_tile = [ (m1*M0+mt, k1*K0+kt) for mt in range(M0) for kt in range(K0)]\n",
    "            b_tile = [ (k1*K0+kt, n1*N0+nt) for kt in range(K0) for nt in range(N0)]\n",
    "\n",
    "            for m0 in range(M0):\n",
    "                for k0 in range(K0):\n",
    "                    for n0 in range(N0):\n",
    "                        m = m1*M0+m0\n",
    "                        n = n1*N0+n0\n",
    "                        k = k1*K0+k0\n",
    "                        \n",
    "                        cache_a.load(getAddress(a, m, k))\n",
    "                        cache_b.load(getAddress(b, k, n))\n",
    "                        cache_c.load(getAddress(z, m, n))\n",
    "                        \n",
    "                        z[m][n] += a[m][k] * b[k][n]\n",
    "\n",
    "                        z_tile = (m, n)\n",
    "                        \n",
    "                        addActivity(canvas, a_tile, b_tile, z_tile, worker=\"W\")\n",
    "                        addFrame(canvas, (m,k), (k,n), (m,n))\n",
    "                        \n",
    "                        cache_c.store(getAddress(z, m, n))\n",
    "\n",
    "print(\"Output - after\")\n",
    "displayTensor(z)\n",
    "\n",
    "displayCanvas(canvas)\n",
    "\n",
    "if z_verify is None:\n",
    "    print(\"Result not verified\")\n",
    "else:\n",
    "    assert z == z_verify\n",
    "    \n",
    "# Print cache statistics\n",
    "print(\"-------Cache A--------\")\n",
    "cache_a.print_stats()\n",
    "print(\"-------Cache B--------\")\n",
    "cache_b.print_stats()\n",
    "print(\"-------Cache C--------\")\n",
    "cache_c.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE ANSWER ALL QUESTIONS WHILE USING THE PROVIDED INITIAL VALUES\n",
    "answer(\n",
    "    question='2.2.1',\n",
    "    subquestion=\"Does the cache for tensor A have (1) compulsory, (2) capacity, and (3) conflict misses? Please write your answer (True/False in Python) in order as a list (e.g., [True, False, True]).\",\n",
    "    answer= ['FILL ME', 'FILL ME', 'FILL ME'], # Answer here\n",
    "    required_type=[bool, bool, bool],\n",
    ")\n",
    "answer(\n",
    "    question='2.2.1',\n",
    "    subquestion=\"Does the cache for tensor B have (1) compulsory, (2) capacity, and (3) conflict misses? Please write your answer (True/False in Python) in order as a list (e.g., [True, False, True]).\",\n",
    "    answer= ['FILL ME', 'FILL ME', 'FILL ME'], # Answer here\n",
    "    required_type=[bool, bool, bool],\n",
    ")\n",
    "answer(\n",
    "    question='2.2.1',\n",
    "    subquestion=\"Does the cache for tensor C have (1) compulsory, (2) capacity, and (3) conflict misses? Please write your answer (True/False in Python) in order as a list (e.g., [True, False, True]).\",\n",
    "    answer= ['FILL ME', 'FILL ME', 'FILL ME'], # Answer here\n",
    "    required_type=[bool, bool, bool],\n",
    ")\n",
    "answer(\n",
    "    question='2.2.1',\n",
    "    subquestion=\"Compared to the untiled matrix multiplication, which of 'compulsory', 'capacity', or 'conflict' misses does tiling reduce?\",\n",
    "    answer= 'FILL ME',\n",
    "    required_type=('compulsory', 'capacity', 'conflict'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand how tiling results in fewer capacity misses, we analyze the *reuse distance* of each tensor. Here, we define reuse distance as *the number of unique tensor elements that are accessed between subsequent access of the same element, including the first access of that element*. For example, if the tensor A has the following accesses:\n",
    "```\n",
    "a[0][0], a[0][1], a[1][0], a[1][1], a[0][0], a[0][1], ...\n",
    "```\n",
    "Then, the element `a[0][0]` has reuse distance of 4. If the cache has capacity larger than 4 and there are no address conflicts, then the element `a[0][0]` is reused; otherwise, it is not reused, and we have a cache miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE ANSWER ALL QUESTIONS WHILE USING THE PROVIDED INITIAL VALUES\n",
    "answer(\n",
    "    question='2.2.2',\n",
    "    subquestion=\"How many times is an element of tensor A accessed?\",\n",
    "    answer= 'FILL ME',\n",
    "    required_type=Number,\n",
    ")\n",
    "answer(\n",
    "    question='2.2.2',\n",
    "    subquestion=\"What is the reuse distance of tensor A in the dataflow in QUESTION 1? Answer with an integer value.\",\n",
    "    answer= 'FILL ME',\n",
    "    required_type=Number,\n",
    ")\n",
    "answer(\n",
    "    question='2.2.2',\n",
    "    subquestion=\"What is the maximum reuse distance of tensor B in the dataflow in QUESTION 1? Answer with an integer value.\",\n",
    "    answer= 'FILL ME',\n",
    "    required_type=Number,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An element of a tensor may be accessed more than twice and the reuse distance between any two accesses may not be the same. This is the case with the tiled dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE ANSWER ALL QUESTIONS WHILE USING THE PROVIDED INITIAL VALUES\n",
    "answer(\n",
    "    question='2.2.3',\n",
    "    subquestion=\"What is the reuse distance of tensor A in the dataflow in QUESTION 2? Answer as a list of [shorter reuse distance, longer reuse distance].\",\n",
    "    answer= ['FILL ME', 'FILL ME'], # Answer here\n",
    "    required_type=[Number, Number],\n",
    ")\n",
    "answer(\n",
    "    question='2.2.3',\n",
    "    subquestion=\"What is the reuse distance of tensor B in the dataflow in QUESTION 2? Answer as a list of [shorter reuse distance, longer reuse distance].\",\n",
    "    answer= ['FILL ME', 'FILL ME'], # Answer here\n",
    "    required_type=[Number, Number],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that tiling involves a trade-off. The new chosen tiling reduces the reuse distance of (some) repeated accesses of tensor B elements. At the same time, (some) reuse distance of repeated accesses of tensor A elements have increased.\n",
    "\n",
    "Because the increased reuse distance of tensor A is still within the capacity of `cache_a` and the shorter reuse distance of tensor B is within the capacity of `cache_b`, this tiling choice results in fewer misses overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Takeaway\n",
    "More data reuse overall (thus, fewer misses) can be achieved by using a carefully chosen tiling to trade-off the reuse distance of one tensor with another tensor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
